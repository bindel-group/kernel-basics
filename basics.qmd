---
title: Kernels and BO in Julia
author: David Bindel
date: 2025-07-23
jupyter: julia-1.11
format:
  html:
    toc: true
  pdf:
    toc: true
    monofont: "Fira Code"
---

```{julia}
#| echo: false
#| output: false
using LinearAlgebra
using Plots
using Random
using StatsFuns
using SpecialFunctions
using Optim
using Test

include("_src/testing.jl")
include("_src/sample.jl")
include("_src/kfuns.jl")
include("_src/kmats.jl")
include("_src/gpp.jl")
include("_src/hypers.jl")
include("_src/acquisition.jl")
include("_src/bo_step.jl")
```

::: {.content-hidden unless-format="html"}
$$
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\bbR}{\mathbb{R}}
$$
:::

::: {.content-hidden unless-format="pdf"}
```{=latex}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\bbR}{\mathbb{R}}
```
:::

# Introduction

Given that there are several of us working on kernel methods and
Bayesian optimization at the moment, it seems worth giving a
bare-bones treatment of the computational pieces.  This is also an
opportunity for me to write about using Julia efficiently (though we
will not worry too much about all the details, nor will we try to
overly optimize the implementation).  And this is an opportunity to write
about my current software philosophy.

The basic moving pieces are:

- Choosing an initial sample
- Approximating a function with a kernel
- Choosing kernel hyperparameters
- Gradients of posterior means and variances
- Some standard acquisition functions
- Optimization of acquisition functions
- Additional numerical tricks

{{< include _ldoc/testing.qmd >}}

{{< include _ldoc/sample.qmd >}}

{{< include _ldoc/kfuns.qmd >}}
{{< include _ldoc/test_kfuns.qmd >}}

{{< include _ldoc/kmats.qmd >}}
{{< include _ldoc/test_kmats.qmd >}}

{{< include _ldoc/gpp.qmd >}}

{{< include _ldoc/hypers.qmd >}}

{{< include _ldoc/acquisition.qmd >}}

{{< include _ldoc/bo_step.qmd >}}
