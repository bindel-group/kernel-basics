# Matrix derivatives

We first briefly recall how variational notation works.  For a given
function $f$, the symbol $\delta f$ (read "variation of $f$")
represents a generic directional derivative with respect to some
underlying parameter.  If $f$ depends on $x$, for example, we would
write $\delta f = f'(x) \, \delta x$.  For second variations, we would
usually use $\Delta$, e.g.
$$
  \Delta \delta f =
  f''(x) \, \delta x \, \Delta x +
  f'(x) \Delta \delta x
$$
The advantage of this notation is
that it sweeps under the rug some of the book-keeping of tracking what
parameter we differentiate with respect to.

## Differentiating through the inverse

To differentiate the NLL, we need to be able to differentiate inverses
and log-determinants.  We begin with inverses.  Applying implicit
differentiation to the equation $A^{-1} A = I$ gives us
$$
  \delta[A^{-1}] \, A + A^{-1} \, \delta A = 0,
$$
which we can rearrange to
$$
  \delta[A^{-1}] = -A^{-1} (\delta A) A^{-1}.
$$
The second derivative is
$$
  \Delta \delta [A^{-1}] =
  A^{-1} (\Delta A) A^{-1} (\delta A) A^{-1} +
  A^{-1} (\delta A) A^{-1} (\Delta A) A^{-1}
  -A^{-1} (\Delta \delta A) A^{-1}
$$

It is a useful habit to check derivative computations with finite
differences, and we will follow that habit here.
```{julia}
let
    A0, δA, ΔA, ΔδA = randn(10,10), rand(10,10), rand(10,10), rand(10,10)

    δinv(A,δA) = -A\δA/A
    invAδA, invAΔA, invAΔδA = A0\δA, A0\ΔA, A0\ΔδA
    ΔδinvA = (invAδA*invAΔA + invAΔA*invAδA - invAΔδA)/A0

    @test δinv(A0,δA) ≈ diff_fd(s->inv(A0+s*δA)) rtol=1e-6
    @test ΔδinvA ≈ diff_fd(s->δinv(A0+s*ΔA, δA+s*ΔδA)) rtol=1e-6
end
```

## Differentiating the log determinant

For the case of the log determinant, it is helpful to decompose a
generic square matrix $F$ as
$$
  F = L + D + U
$$
where $L$, $D$, and $U$ are the strictly lower triangular, diagonal,
and strictly upper triangular parts of $F$, respectively.  Then note
that
$$
  (I+\epsilon F) = (I + \epsilon L)(I + \epsilon (D+U)) + O(\epsilon^2),
$$
and therefore
$$\begin{aligned}
  \det(I+\epsilon F)
  &= \det(I + \epsilon(D+U)) + O(\epsilon^2) \\
  &= \prod_i (1+ \epsilon d_i) + O(\epsilon^2) \\
  &= 1 + \epsilon \sum_i d_i + O(\epsilon^2) \\
  &= 1 + \epsilon \tr(F) + O(\epsilon^2).
\end{aligned}$$
Hence the derivative of $\det(A)$ about $A = I$ is $\tr(A)$.

Now consider
$$
  \det(A + \epsilon (\delta A)) =
  \det(A) \det(I+ \epsilon A^{-1} \delta A) =
  \det(A) + \epsilon \det(A) \tr(A^{-1} \delta A) + O(\epsilon^2).
$$
This gives us that in general,
$$
  \delta[\det(A)] = \det(A) \tr(A^{-1} \delta A),
$$
and hence
$$
  \delta[\log \det(A)]
  = \frac{\delta[\det(A)]}{\det(A)}
  = \tr(A^{-1} \delta A).
$$
We can also write this as
$$
  \delta[\log \det(A)]
  = \langle A^{-T}, \delta A \rangle_F,
$$
i.e. $A^{-T}$ is the gradient of $\log \det(A)$.

The second derivative is
$$
  \Delta \delta [\log \det(A)] =
  \tr(A^{-1} \Delta \delta A) -
  \tr(A^{-1} \Delta A \, A^{-1} \delta A).
$$

Again, a finite difference check is a useful thing.  We do need to
be a little careful here in order to make sure that the log
determinant is well defined at $A$ and in a near neighborhood.
```{julia}
let
    V = randn(10,10)
    A = V*Diagonal(1.0.+rand(10))/V
    δA, ΔA, ΔδA = randn(10,10), randn(10,10), randn(10,10)

    δlogdet(A, δA) = tr(A\δA)
    Δδlogdet(A, δA, ΔA, ΔδA) = tr(A\ΔδA)-tr((A\ΔA)*(A\δA))

    @test δlogdet(A,δA) ≈ dot(inv(A'), δA)
    @test δlogdet(A,δA) ≈ diff_fd(s->log(det(A+s*δA))) rtol=1e-6
    @test Δδlogdet(A,δA,ΔA,ΔδA) ≈ diff_fd(s->δlogdet(A+s*ΔA,δA+s*ΔδA)) rtol=1e-6
end
```
